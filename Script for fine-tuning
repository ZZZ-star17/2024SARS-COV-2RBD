# Install required libraries and resolve dependency conflicts
!pip install biopython
!pip install datasets
!pip uninstall -y fsspec gcsfs
!pip install fsspec==2024.10.0 gcsfs==2024.10.0
!pip install transformers datasets evaluate nltk tqdm -q

# Clone the VirusT5 repository
!git clone https://github.com/vrmarathe/VirusT5.git
%cd VirusT5

# Import necessary libraries
import pandas as pd
import numpy as np
from datasets import Dataset
from transformers import AutoTokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer
from transformers import DataCollatorForSeq2Seq
from tqdm import tqdm
import nltk
import evaluate

# Ensure nltk dependencies are available
nltk.download("punkt", quiet=True)

# Load the pre-trained VirusT5 model and tokenizer
checkpoint = "vrmarathe/VirusT5"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = T5ForConditionalGeneration.from_pretrained(checkpoint, from_flax=True)

# Set a reasonable max_length for the model
max_length = 680  # Set to 512 tokens as a safe value

# Load the RBD nucleotide and protein sequence data
data_path = "/content/2024_output_data_lineage.csv"
rbd_df = pd.read_csv(data_path)

# Combine Seq1 and Seq2 into a single column for translation
rbd_df['translation'] = rbd_df.apply(lambda row: {'Seq1': row['Seq1'], 'Seq2': row['Seq2']}, axis=1)

# Drop unnecessary columns
rbd_df = rbd_df.drop(['Seq1', 'Seq2', 'Lineage', 'GenS1', 'GenS2'], axis=1, errors='ignore')

# Convert to Hugging Face Dataset format
hf_dataset = Dataset.from_pandas(rbd_df)
hf_dataset = hf_dataset.train_test_split(train_size=0.8, seed=42)

# Further split the training set into training and validation sets
hf_clean = hf_dataset["train"].train_test_split(train_size=0.8, seed=42)
hf_clean["validation"] = hf_clean.pop("test")
hf_clean["test"] = hf_dataset["test"]

# Define preprocessing function
prefix = "translate Seq1 to Seq2: "

def preprocess_function(examples):
    # Access "translation" field properly
    inputs = [prefix + seq["Seq1"] for seq in examples["translation"]]
    targets = [seq["Seq2"] for seq in examples["translation"]]
    return tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True)

# Tokenize the dataset
tokenized_hf = hf_clean.map(preprocess_function, batched=True)

# Define data collator for batching
data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, max_length=max_length)

# Load evaluation metric
metric = evaluate.load("bleu")

# Define metric computation
def compute_metrics(eval_preds):
    preds, labels = eval_preds
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    return metric.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])

# Training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="/content/virusT5_finetuned",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=64,
    per_device_eval_batch_size=64,
    weight_decay=0.01,
    num_train_epochs=8,
    predict_with_generate=True,
    fp16=True,  # Enable FP16 for A100 GPUs
    logging_dir="./logs",
    logging_steps=200,
)

# Define Seq2SeqTrainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_hf["train"],
    eval_dataset=tokenized_hf["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# Train the model
print("Starting training...")
trainer.train()

# Evaluate the model on the test set
print("Evaluating the model...")
results = trainer.evaluate(eval_dataset=tokenized_hf["test"])
print("Test Evaluation Results:", results)

# Save the model and tokenizer
print("Saving the model and tokenizer...")
model.save_pretrained("/content/virusT5_2024finetuned")
tokenizer.save_pretrained("/content/virusT5_2024finetuned")
