!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install transformers datasets evaluate biopython nltk tqdm

import pandas as pd
import numpy as np
import nltk
from transformers import AutoTokenizer
import warnings

# Disable tqdm warning messages
warnings.filterwarnings("ignore", category=UserWarning)

# Load the data
df = pd.read_csv("/content/2024_output_data_lineage.csv")  # Updated path
print("Original Dataset !! ")
print(df.head())

df = df.dropna()

print("\nColumns:", df.columns)

# Data Preprocessing
df.drop(['Lineage', 'GenS1', 'GenS2'], axis=1, inplace=True)
df['translation'] = df.apply(lambda row: {'Seq1': row['Seq1'], 'Seq2': row['Seq2']}, axis=1)
print(df['translation'][0])
df = df.drop(['Seq1', 'Seq2', 'Variant'], axis=1)

from datasets import Dataset
hf = Dataset.from_pandas(df)
hf = hf.train_test_split(train_size=0.8, seed=42)
hf_clean = hf["train"].train_test_split(train_size=0.8, seed=42)
hf_clean["validation"] = hf_clean.pop("test")
hf_clean["test"] = hf["test"]
hf = hf_clean

print("\n\nDataset after Breaking into Train_Val_Test")
print(hf)

# Tokenizer and Model Setup
source_lang = "Seq1"
target_lang = "Seq2"
prefix = "translate Seq1 to Seq2: "

checkpoint = "vrmarathe/VirusT5"  # Updated to VirusT5 Hugging Face model
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

def preprocess_function(examples):
    inputs = [prefix + example[source_lang] for example in examples["translation"]]
    targets = [example[target_lang] for example in examples["translation"]]
    model_inputs = tokenizer(inputs, text_target=targets, max_length=667, truncation=False)
    return model_inputs

tokenized_hf = hf.map(preprocess_function, batched=True)

from transformers import DataCollatorForSeq2Seq
data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, max_length=667)

from transformers import T5ForConditionalGeneration
model = T5ForConditionalGeneration.from_pretrained(checkpoint, from_flax=True)  # Updated with `from_flax=True`

from transformers import AdamWeightDecay
optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)

# Evaluation Metric
import evaluate
metric = evaluate.load("bleu")

def seq2kmer(seq, k):
    kmer = [seq[x:x + k] for x in range(len(seq) + 1 - k)]
    kmers = " ".join(kmer)
    return kmers

def compute_metrics(eval_preds):
    preds, labels = eval_preds
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    df_predictions = pd.DataFrame({'Labels': decoded_labels, 'Predictions': decoded_preds})
    print(df_predictions.head())
    df_predictions.to_csv("df_predictions_final.csv")
    result = metric.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])
    return result

from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer
training_args = Seq2SeqTrainingArguments(
    output_dir="/content/virusT5_finetuned",  # Updated path
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=12,
    per_device_eval_batch_size=10,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=4,
    predict_with_generate=True,
    fp16=True,
    generation_max_length=667,
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_hf["train"],
    eval_dataset=tokenized_hf["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# Training and Evaluation
trainer.train()
results = trainer.evaluate(eval_dataset=tokenized_hf["test"])

print("\n\n RESULTS OF TEST DATASET !!")
print("\n\n")
print(results)

